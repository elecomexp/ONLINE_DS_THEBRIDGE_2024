{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagen](img/foto1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagen](img/foto2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFdOWOUXhEPg"
   },
   "source": [
    "Para ejercitarte y afianzar lo aprendido sobre **Embeddings y procesamiento de texto**, completa los siguientes ejercicios. Recuerda que necesitarás datos que están en el directorio data que acompaña al notebook (búscalo en el repositorio de ejercicios).\n",
    "\n",
    "La solución a los mismos las tienes ya, y en este caso se te invita a QUE SIGAS EL EJERCICIO CON LA SOLUCION a modo de tutorial ya que hay varios aspectos que son nuevos y se introducen en la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": [
    "### Ejercicio 0\n",
    "\n",
    "Importa las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Embedding, GlobalAveragePooling1D,\n",
    "                                     TextVectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### Ejercicio 1: Descarga el dataset\n",
    "\n",
    "Usarás el [Conjunto de Datos de Grandes Reseñas de Películas](http://ai.stanford.edu/~amaas/data/sentiment/) a lo largo del tutorial. Entrenarás un modelo de clasificador de sentimientos con este conjunto de datos y, en el proceso, aprenderás embeddings desde cero. \n",
    "\n",
    "Descarga el conjunto de datos utilizando la utilidad de archivos de Keras y echa un vistazo a los directorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=url,\n",
    "                                  extract=True, \n",
    "                                  cache_dir='./data/',\n",
    "                                  cache_subdir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeV_Ke-6hEPi",
    "outputId": "76ab7da0-be5a-4b42-c63a-de8b40cd858f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/./aclImdb_v1_extracted\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoegeWiGhEPi",
    "outputId": "5cd2d5c9-f4b0-49dc-8ec2-44de5a0353ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lander\\Documents\\GitHub\\ONLINE_DS_THEBRIDGE_2024\\06_Deep_Learning\\S20_Redes_Recurrentes_y_Embeddings\\U02_Capas_Prepocesado_y_Embedings\\Ejercicios_Workout\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STHEMjifhEPi"
   },
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "Echa un vistazo al directorio train/. Tiene carpetas pos y neg con reseñas de películas etiquetadas como positivas y negativas respectivamente. Utilizarás reseñas de las carpetas pos y neg para entrenar un modelo de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-iOHJGN6SDu",
    "outputId": "72308dc4-5198-43df-a079-e8c8c88a8765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = './data/aclImdb_v1_extracted/aclImdb'\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCs4BbllhEPj"
   },
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "El directorio `train` también tiene carpetas adicionales que deberían ser eliminadas antes de crear el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OV4ARuRphEPj"
   },
   "outputs": [],
   "source": [
    "# delete path\n",
    "shutil.rmtree(dataset_dir + \"/train/unsup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay45Npg8hEPj"
   },
   "source": [
    "### Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "A continuación, crea un tf.data.Dataset usando tf.keras.preprocessing.text_dataset_from_directory. Puedes leer más sobre cómo utilizar esta utilidad en este [tutorial de clasificación de texto](https://www.tensorflow.org/tutorials/keras/text_classification).\n",
    "\n",
    "Usa el directorio de entrenamiento para crear conjuntos de datos de entrenamiento y validación con una división del 20% para la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItYD3TLkCOP1",
    "outputId": "74f7c287-c20d-41f2-a5b1-dd04bfe366d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "'''\n",
    "Busca en un directorio todas las carpetas. Cada carpeta es una etiqueta\n",
    "Y cada archivo una review. Podemos especificar si es para el subset\n",
    "de training o validation y cuanto dejamos para validacion.\n",
    "Esto crea un tf.data.Dataset\n",
    "ELIMINAR UNA CARPETA EN TRAIN, QUE SOBRA\n",
    "'''\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation', # Esto y la semilla permiten que las muestras con train no se superpongan\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: no se si esta forma también sería correcta\n",
    "\n",
    "# batch_size = 1024\n",
    "# seed = 123\n",
    "\n",
    "# '''\n",
    "# Busca en un directorio todas las carpetas. Cada carpeta es una etiqueta\n",
    "# Y cada archivo una review. Podemos especificar si es para el subset\n",
    "# de training o validation y cuanto dejamos para validacion.\n",
    "# Esto crea un tf.data.Dataset\n",
    "# ELIMINAR UNA CARPETA EN TRAIN, QUE SOBRA\n",
    "# '''\n",
    "\n",
    "# train_ds, val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#     train_dir,\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset='both',\n",
    "#     seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "*Echa un vistazo a algunas reseñas de películas y sus etiquetas (1: positiva, 0: negativa) del conjunto de datos de entrenamiento.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTCbSkvkYmTT",
    "outputId": "0f12d7bd-8fdb-4b6c-8d2c-47a5fa55a833",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have st' ...\n",
      "Label: 0 (Negative)\n",
      "b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams' ...\n",
      "Label: 1 (Positive)\n",
      "b'Alex D. Linz replaces Macaulay Culkin as the central figure in the third movie in the Home Alone empire. Four industrial spies acquire a missile guida' ...\n",
      "Label: 0 (Negative)\n",
      "b\"There's a good movie lurking here, but this isn't it. The basic idea is good: to explore the moral issues that would face a group of young survivors o\" ...\n",
      "Label: 0 (Negative)\n",
      "b'I saw this movie at an actual movie theater (probably the $2.00 one) with my cousin and uncle. We were around 11 and 12, I guess, and really into scar' ...\n",
      "Label: 0 (Negative)\n"
     ]
    }
   ],
   "source": [
    "# Cogemos el primer batch\n",
    "for text_batch, label_batch in train_ds.take(1): \n",
    "    for i in range(5): # 1025 da error xq no hay mas en este batch\n",
    "        print(text_batch.numpy()[i][:150], \"...\")\n",
    "        print(\"Label:\", label_batch[i].numpy(), \"(%s)\" %(\"Positive\" if label_batch.numpy()[i] == 1 else \"Negative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### Ejercicio 5: Configura el dataset para mejorar el rendimiento (mira la solución)\n",
    "\n",
    "Estos son dos métodos importantes que deberías usar al cargar datos para asegurarte de que las operaciones de entrada/salida no se conviertan en un bloqueo.\n",
    "\n",
    "`.cache()` mantiene los datos en memoria después de ser cargados desde el disco. Esto garantizará que el conjunto de datos no se convierta en un cuello de botella mientras entrenas tu modelo. Si tu conjunto de datos es demasiado grande para caber en la memoria, también puedes usar este método para crear una caché en disco eficiente, la cual es más eficiente para leer que muchos archivos pequeños.\n",
    "\n",
    "`.prefetch()` solapa el preprocesado de datos y la ejecución del modelo durante el entrenamiento.\n",
    "\n",
    "Puedes aprender más sobre ambos métodos, así como cómo cachear datos en disco en la [guía de rendimiento de datos](https://www.tensorflow.org/guide/data_performance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "### Ejercicio 6: Repasando Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3_gGUTRhEPk"
   },
   "source": [
    "Crea una capa de Embedding con dimensión de entrada 1000 y dimensión de salida 4. Codifica las frases \"Me llamo Iñigo Montoya\", \"Tú mataste a mi padre\", \"Disponte a morir\" (tendrás que crear una capa adicional). Haz la codificación de cada una por separado y luego prueba a ponerlas todas juntas en una misma lista. Ojo, cada frase la tienes que convertir a una lista de palabras. ¿Qué ocurre en este último caso? [Haz el ejercicio y luego mira la solución]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Me', 'llamo', 'Iñigo', 'Montoya', 'Tú', 'mataste', 'a', 'mi', 'padre', 'Disponte', 'a', 'morir']\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Me llamo Iñigo Montoya\", \"Tú mataste a mi padre\", \"Disponte a morir\"]\n",
    "print(\" \".join(frases).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1j3WOI2ghEPk",
    "outputId": "bb089035-4a2e-40b3-d8a5-3a6e2125abe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para <Me llamo Iñigo Montoya>, embeddings:\n",
      "[[[ 0.04838357 -0.02706586 -0.01583712 -0.01494152]\n",
      "  [ 0.03206224  0.03040278  0.01336911  0.03139034]\n",
      "  [ 0.01332668  0.04663992  0.01068345 -0.01346766]\n",
      "  [ 0.04013517 -0.01368649  0.02875055 -0.04734081]]]\n",
      "Para <Tú mataste a mi padre>, embeddings:\n",
      "[[[-0.04695284 -0.04246197 -0.03199855  0.0075048 ]\n",
      "  [-0.0225799  -0.03721614 -0.00847229 -0.00089647]\n",
      "  [ 0.0371753   0.00765828  0.02573992  0.01082981]\n",
      "  [-0.04406366 -0.01233744  0.00857102  0.01664624]\n",
      "  [-0.04881733 -0.01097306 -0.00337977  0.0275001 ]]]\n",
      "Para <Disponte a morir>, embeddings:\n",
      "[[[-0.01865578 -0.02029446  0.02361711 -0.02638083]\n",
      "  [ 0.0371753   0.00765828  0.02573992  0.01082981]\n",
      "  [ 0.02794869 -0.04948401  0.00572672 -0.03148983]]]\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Me llamo Iñigo Montoya\", \"Tú mataste a mi padre\", \"Disponte a morir\"]\n",
    "\n",
    "layer_test = Embedding(input_dim=1000, output_dim=4)\n",
    "pre_procesado = tf.keras.layers.StringLookup()\n",
    "pre_procesado.adapt(\" \".join(frases).split())\n",
    "\n",
    "conversor_fake = tf.keras.models.Sequential([pre_procesado,\n",
    "                                             layer_test])\n",
    "\n",
    "resultados = []\n",
    "for frase in frases:\n",
    "    resultados.append(conversor_fake(tf.constant([frase.split()])))\n",
    "    print(f\"Para <{frase}>, embeddings:\\n{resultados[-1].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcei85jdhEPk"
   },
   "source": [
    "*Para problemas de texto o secuencias, la capa Embedding toma un tensor 2D de enteros, de forma (muestras, longitud_de_secuencia), donde cada entrada es una secuencia de enteros. Puede incrustar secuencias de longitudes variables. Podrías alimentar la capa de incrustación con lotes de formas (32, 10) (lote de 32 secuencias de longitud 10) o (64, 15) (lote de 64 secuencias de longitud 15). Como se ve al meter frasess de diferente tamaño POR SEPARADO, pero fijate en cuando pruebas con todas las frases juntas.*\n",
    "\n",
    "*Por otro lado El tensor devuelto tiene un eje más que la entrada, los vectores de incrustación se alinean a lo largo del nuevo último eje. Pásale un lote de entrada (2, 3) y la salida es (2, 3, N). Es decir, conjuntando lo anterior y este punto, si le pasas secuencias de diferente tamaño te devuelve secuencias de diferente tamaño y eso tendremos que \"ajustarlo\" para las siguientes capas. Ahora hagamos el intento de lanzar todo junto:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "M3pgNXFhhEPl",
    "outputId": "ce258717-0212-42d4-b374-c24c482beb43"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conversor_fake(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfrase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfrase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfrases\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Lander\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\Lander\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lander\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32mc:\\Users\\Lander\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\Lander\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "conversor_fake(tf.constant([frase.split() for frase in frases]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iLb1I-YhEPl"
   },
   "source": [
    "*Los tensores son estructuras \"rectangulares\" eso de meterle secuencias de longitud variable nos lleva a un primer punto que debéis conocer si vais a trabajar con NLP en DL... __Es necesario hacer Padding__, es decir hacer secuencias de un tamaño fijo rellenando con \"ceros\" las secuencias más cortas.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQGSgaRQhEPl",
    "outputId": "c7a3dd50-4efe-4efb-9e80-f8d8a93a7c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "['Me', 'llamo', 'Iñigo', 'Montoya'] 4 5\n",
      "['Me', 'llamo', 'Iñigo', 'Montoya', '<relleno>']\n",
      "['Tú', 'mataste', 'a', 'mi', 'padre'] 5 5\n",
      "['Tú', 'mataste', 'a', 'mi', 'padre']\n",
      "['Disponte', 'a', 'morir'] 3 5\n",
      "['Disponte', 'a', 'morir', '<relleno>', '<relleno>']\n",
      "\n",
      "Frases padded:\n",
      "[['Me', 'llamo', 'Iñigo', 'Montoya', '<relleno>'], ['Tú', 'mataste', 'a', 'mi', 'padre'], ['Disponte', 'a', 'morir', '<relleno>', '<relleno>']]\n",
      "tf.Tensor(\n",
      "[[b'Me' b'llamo' b'I\\xc3\\xb1igo' b'Montoya' b'<relleno>']\n",
      " [b'T\\xc3\\xba' b'mataste' b'a' b'mi' b'padre']\n",
      " [b'Disponte' b'a' b'morir' b'<relleno>' b'<relleno>']], shape=(3, 5), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5, 4), dtype=float32, numpy=\n",
       "array([[[ 0.04838357, -0.02706586, -0.01583712, -0.01494152],\n",
       "        [ 0.03206224,  0.03040278,  0.01336911,  0.03139034],\n",
       "        [ 0.01332668,  0.04663992,  0.01068345, -0.01346766],\n",
       "        [ 0.04013517, -0.01368649,  0.02875055, -0.04734081],\n",
       "        [-0.04411323, -0.02603287,  0.00755753,  0.04467168]],\n",
       "\n",
       "       [[-0.04695284, -0.04246197, -0.03199855,  0.0075048 ],\n",
       "        [-0.0225799 , -0.03721614, -0.00847229, -0.00089647],\n",
       "        [ 0.0371753 ,  0.00765828,  0.02573992,  0.01082981],\n",
       "        [-0.04406366, -0.01233744,  0.00857102,  0.01664624],\n",
       "        [-0.04881733, -0.01097306, -0.00337977,  0.0275001 ]],\n",
       "\n",
       "       [[-0.01865578, -0.02029446,  0.02361711, -0.02638083],\n",
       "        [ 0.0371753 ,  0.00765828,  0.02573992,  0.01082981],\n",
       "        [ 0.02794869, -0.04948401,  0.00572672, -0.03148983],\n",
       "        [-0.04411323, -0.02603287,  0.00755753,  0.04467168],\n",
       "        [-0.04411323, -0.02603287,  0.00755753,  0.04467168]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tam_maximo = max([len(frase.split()) for frase in frases])\n",
    "print(tam_maximo, end='\\n\\n')\n",
    "\n",
    "frases_padded = []\n",
    "for frase in frases:\n",
    "    sentence = frase.split()\n",
    "    print(sentence, len(sentence), tam_maximo)\n",
    "    sentence += [\"<relleno>\" for i in range(len(sentence),tam_maximo)]\n",
    "    print(sentence)\n",
    "    frases_padded.append(sentence)\n",
    "\n",
    "print('\\nFrases padded:', frases_padded, sep='\\n')\n",
    "print(tf.constant(frases_padded))\n",
    "conversor_fake(tf.constant(frases_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf8zjh36hEPl"
   },
   "source": [
    "*Esto resuelve dos cosas: el hecho de que podamos meter las frases como batches completos, y la salida que también está fija, pero traerá otro problema, la descompensación entre frases largas y cortas, ya que en estas últimas habrá mucho \"relleno\". La forma de solucionarlo se llama \"masking\" o enmascaramiento y se hará usando una máscara por secuencia para indicarle a las capas qué valores son de relleno y no se han de usar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "*Además del padding, para convertir de esta secuencia de longitud variable a una representación fija, hay una variedad de enfoques estándar. Podrías usar una capa RNN, Atención, o de agrupación antes de pasarla a una capa Densa. En estos ejercicios vamos a usar la agrupación con una capa de Pooling.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "### Ejercicio 7: Procesado de Texto con vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "A continuación, define los pasos de preprocesamiento del conjunto de datos necesarios para tu modelo de clasificación de sentimientos. Inicializa una capa de TextVectorization con los parámetros deseados para vectorizar las reseñas de películas. Recuerda que tendrás que limipiar (tambien se llama \"estandandizar\") las reseñas (como lo hicimos en su día o como en el workout o como lo hacemos, diferente, en la solución).\n",
    "\n",
    "Importante: En este ejercicio vamos a usar la capa de vectorización pero no para convertir las frases en conteos de palabras o sus tfidf, sino en listas de indices en un vocabulario. Por eso a la hora de inicializar la capa debes usar el output_mode = \"int\" (en el workout lo hemos empleado con \"count\" y con \"tf-idf\". Configura la capa de forma que ajuste su salida a secuencias o frases de 8 palabras.\n",
    "\n",
    "Prueba la nueva capa con las frases del ejercicio anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# Creamos una función de limpieza\n",
    "def custom_cleaner(input_data):\n",
    "    \"\"\"\n",
    "    Función de limpieza de datos.\n",
    "    Convierte a minúsculas, quita los codigos de cambio de línea, y quita los signos de puntuación\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation), \n",
    "                                    '')\n",
    "\n",
    "\n",
    "sequence_length = 8\n",
    "\n",
    "# Veamos como la capa TextVectorization nos permite darle secuenccias de entrada y\n",
    "# nos devuelve el formato lista de índices, CON PADDING\n",
    "# el modo int eso hace que devuelva una secuencia con índices al vocabulario IMPORTANTE!!!\n",
    "# output_sequence_length --> indica el tamaño de la secuencia de salida y con eso ya estamos diciéndole como queremos hacer el Padding.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_cleaner,\n",
    "    output_mode='int', \n",
    "    output_sequence_length=sequence_length\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BE_xF3DHhEPm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 8), dtype=int64, numpy=\n",
       "array([[ 8, 10, 11,  6,  0,  0,  0,  0],\n",
       "       [ 3,  9,  2,  7,  4,  0,  0,  0],\n",
       "       [12,  2,  5,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit de las frases\n",
    "vectorize_layer.adapt(frases)\n",
    "vectorize_layer(frases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcvx3r-ehEPm"
   },
   "source": [
    "*Fijate como ha hecho el padding hasta llegar a los 8 terminos que le hemos dado como sequence_length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-jHwqjdhEPq",
    "outputId": "5af25bfb-799f-4cd3-d7e0-c46437e2670f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> \n",
      "1 --> [UNK]\n",
      "2 --> a\n",
      "3 --> tú\n",
      "4 --> padre\n",
      "5 --> morir\n",
      "6 --> montoya\n",
      "7 --> mi\n",
      "8 --> me\n",
      "9 --> mataste\n",
      "10 --> llamo\n",
      "11 --> iñigo\n",
      "12 --> disponte\n"
     ]
    }
   ],
   "source": [
    "for indice, word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    print(\"%d --> %s\" %(indice,word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmLbPzIrhEPq"
   },
   "source": [
    "El índice 0 se suele reservar para hacer el padding, como puedes observar en el resultado al aplicar la capa a las frases del ejercicio anterior.\n",
    "\n",
    "Ahora a la salida del TextVectorization le aplicaremos la capa de Embeddings y luego a eso nuestras capas densas para hacer el modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-hwoKhohEPr"
   },
   "source": [
    "### Ejercicio 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI-I8FPohEPr"
   },
   "source": [
    "Recrea la capa del ejercicio anterior, llamándola vectorizer_layer. pero esta vez para un vocabulario de 1000 términos y para un tamaño de secuencia de 100 palabras. LUEGO ejecuta el código que tienes en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y864gxP8hEPr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_standardization(input_data):\n",
    "    \"\"\"\n",
    "    Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "    Lo convierte a minúsculas, le quita los codigos de cambio de línea, y le quita los signos de puntuación\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ') \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation), \n",
    "                                    '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "# Vamos a permitir que tenga un vocabulario de 10000, las de mayor frecuencia \n",
    "# el resto se codificará como UNK, o si hemos empleado OOV como esas categorías extra\n",
    "vocab_size = 10000 \n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ZNawvXhEPr"
   },
   "source": [
    "**Para ejecutar después de la creación de la capa vectorize_layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Qb75R6jdhEPr"
   },
   "outputs": [],
   "source": [
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "# Nos quitamos los labels de esta manera, que van en conjunto con features en train_ds\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unc1IQLYhEPr"
   },
   "source": [
    "### Ejercicio 9\n",
    "\n",
    "Es hora de crear el modelo de clasificación. Tendrás que emplear una capa [\"GlobalAveragePooling1D\"](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) e investigar por tu cuenta un poco sobre ella, aunque aquí te dejo algunos apuntes. En definitiva, tu modelo tiene que tener:\n",
    "\n",
    "* La capa [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) transforma cadenas en índices de vocabulario. Ya has inicializado `vectorize_layer` como una capa de TextVectorization y has construido su vocabulario llamando a `adapt` en `text_ds`. Ahora, vectorize_layer puede ser utilizada como la primera capa de tu modelo de clasificación de principio a fin, alimentando cadenas transformadas en la capa de Embedding. Utiliza una dimensión de salida de 16 en el embedding.\n",
    "  \n",
    "* La capa [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) toma el vocabulario codificado en enteros y busca el vector de incrustación para cada índice de palabra. Estos vectores se aprenden a medida que el modelo se entrena. Los vectores añaden una dimensión al arreglo de salida. Las dimensiones resultantes son: `(lote, secuencia, incrustación)`.\n",
    "\n",
    "* La capa [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) devuelve un vector de salida de longitud fija para cada ejemplo promediando sobre la dimensión de secuencia. Esto permite al modelo manejar entrada de longitud variable, de la manera más simple posible. Esta capa hace el sentence embedding que comentamos en el workout, es decir convierte la sentencia en un embedding que es el resultado de hacer la media de cada uno de sus word embeddings (sí es el centroide de sus embeddings) ya que es la forma más sencilla de hacer sentence embedding.\n",
    "\n",
    "* El vector de salida de longitud fija se pasa a través de una capa completamente conectada ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) con 16 unidades ocultas. (porque tenemos 16 de dimensión del embedding de palabra y de sentencia).\n",
    "\n",
    "* La última capa está densamente conectada con un único nodo de salida (por ser un clasificador binario).\n",
    "\n",
    "Precaución: Este modelo no utiliza enmascaramiento, por lo que el relleno de ceros se utiliza como parte de la entrada y, por lo tanto, la longitud del relleno puede afectar la salida. Para solucionar esto, consulta la [guía de enmascaramiento y relleno](https://www.tensorflow.org/guide/keras/masking_and_padding). Básicamente, al hacer el sentence embedding en frases cortas (imaginate una de una sola palabra, tendrá en nuestro caso 1 word-embedding y 99 de relleno) el embedding de la sentencia se ve completamente sesgado hacia el de relleno, por eso se usa el masking para decirle a la capa que sólo tenga en cuenta el embedding que no sea de relleno.y relleno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 \n",
    "embedding_dim = 16\n",
    "\n",
    "'''\n",
    "GlobalAveragePooling1D\n",
    "Cada palabra tiene asociado un embedding. El ouput es la media de cada\n",
    "coordenada del embedding, por tanto, si hay 16 embeddings, hará un\n",
    "flatten a 16, siendo cada valor la media de la coordenada de ese\n",
    "embedding para todas las palabras de la review\n",
    "'''\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,                                            # 100 [1, 3, 4, 4, 90, ...]\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),     # 10.000 x 16 --> [[], [], [] ...] 100x16\n",
    "    GlobalAveragePooling1D(),                                   # [] 16\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation=\"sigmoid\")                              # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "### Ejercicio 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Entrena el modelo usando un optimizador \"Adam\" y la función de perdida \"BinaryCrossEntropy\". Evalua contra test. Prueba con 15 épocas (es muy pesado el entrenamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/61233425/what-should-i-use-as-target-vector-when-i-use-binarycrossentropyfrom-logits-tru\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mQehiQyv8rP",
    "outputId": "2f9d91a0-1718-4eaf-9235-617630bf5f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 654ms/step - accuracy: 0.5702 - loss: 0.6922 - val_accuracy: 0.6370 - val_loss: 0.6875\n",
      "Epoch 2/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.6719 - loss: 0.6852 - val_accuracy: 0.6790 - val_loss: 0.6769\n",
      "Epoch 3/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.6949 - loss: 0.6733 - val_accuracy: 0.7038 - val_loss: 0.6607\n",
      "Epoch 4/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7168 - loss: 0.6546 - val_accuracy: 0.7330 - val_loss: 0.6379\n",
      "Epoch 5/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7426 - loss: 0.6289 - val_accuracy: 0.7514 - val_loss: 0.6094\n",
      "Epoch 6/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7629 - loss: 0.5971 - val_accuracy: 0.7674 - val_loss: 0.5773\n",
      "Epoch 7/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7828 - loss: 0.5615 - val_accuracy: 0.7818 - val_loss: 0.5446\n",
      "Epoch 8/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7969 - loss: 0.5255 - val_accuracy: 0.7938 - val_loss: 0.5140\n",
      "Epoch 9/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8075 - loss: 0.4917 - val_accuracy: 0.8042 - val_loss: 0.4872\n",
      "Epoch 10/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8171 - loss: 0.4615 - val_accuracy: 0.8092 - val_loss: 0.4646\n",
      "Epoch 11/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8244 - loss: 0.4353 - val_accuracy: 0.8174 - val_loss: 0.4459\n",
      "Epoch 12/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8321 - loss: 0.4128 - val_accuracy: 0.8212 - val_loss: 0.4307\n",
      "Epoch 13/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8408 - loss: 0.3934 - val_accuracy: 0.8238 - val_loss: 0.4183\n",
      "Epoch 14/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8479 - loss: 0.3765 - val_accuracy: 0.8242 - val_loss: 0.4082\n",
      "Epoch 15/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8536 - loss: 0.3616 - val_accuracy: 0.8250 - val_loss: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e559edb110>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yK48BiWhEPs",
    "outputId": "8844427c-9748-4b37-dc1d-4c9f5ae5d3d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    dataset_dir + \"/test/\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA-GcvYzhEPs",
    "outputId": "1061b61b-f37a-437d-c0f6-1a3d5a8813f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.8059 - loss: 0.4230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4163852632045746, 0.8100000023841858]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmQnbW1-hEPs"
   },
   "source": [
    "### Bonus: Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmutnneUhEPt"
   },
   "source": [
    "Al limitar el tamaño de secuencia a 100, el modelo está intencionadamente evitando problemas de excesivo padding (las reviews seguramente son de más de 100 palabras), pero...  \n",
    "El coste es perder capacidad, porque a las 100 palabras ya toma una decisión.  \n",
    "Si quisieramos que leyese más de 100 palabras o todas, seguramente tendríamos secuencias con mucho padding, y para evitar el problema de este, usaríamos máscaras, con sólo cambiar esto (en la versión fácil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9tB_bypkhEPt"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer,              # 100 [1, 3, 4, 4, 90, ...]\n",
    "  Embedding(vocab_size, \n",
    "            embedding_dim, \n",
    "            name=\"embedding\", \n",
    "            mask_zero=True),    # la capa  Embedding admite mask, le estamos diciendo que si ve un zero lo ignore\n",
    "  GlobalAveragePooling1D(),     # [] 16\n",
    "  Dense(16, activation='relu'), \n",
    "  Dense(1)                      # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI4z3bTihEPt"
   },
   "source": [
    "Además la capa de Embedding le pasa la mascara a la siguiente capa (en este caso la GlobalAveragePooling1D) a través del argumento mask. Si la capa tiene ese argumento recibe la máscar y la trata como tenga programado.  \n",
    "En este caso GlobalAveragePooling1D ignora el elemento enmascarado y no lo incluye en la media.   \n",
    "\n",
    "Una capa recurrente si recibe una máscara ignora los elementos a 0 y para ese \"time_step\" para ese valor repite el último valor de hidden_state válido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.5037 - loss: 2.8781 - val_accuracy: 0.4886 - val_loss: 1.2351\n",
      "Epoch 2/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5037 - loss: 1.1370 - val_accuracy: 0.4886 - val_loss: 1.0149\n",
      "Epoch 3/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.5037 - loss: 0.9578 - val_accuracy: 0.4886 - val_loss: 0.8947\n",
      "Epoch 4/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.5037 - loss: 0.8540 - val_accuracy: 0.4886 - val_loss: 0.8126\n",
      "Epoch 5/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5037 - loss: 0.7828 - val_accuracy: 0.4886 - val_loss: 0.7555\n",
      "Epoch 6/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5039 - loss: 0.7342 - val_accuracy: 0.4890 - val_loss: 0.7173\n",
      "Epoch 7/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.5063 - loss: 0.7022 - val_accuracy: 0.5008 - val_loss: 0.6924\n",
      "Epoch 8/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5318 - loss: 0.6813 - val_accuracy: 0.5520 - val_loss: 0.6755\n",
      "Epoch 9/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6004 - loss: 0.6664 - val_accuracy: 0.6230 - val_loss: 0.6621\n",
      "Epoch 10/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6637 - loss: 0.6535 - val_accuracy: 0.6830 - val_loss: 0.6497\n",
      "Epoch 11/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.7134 - loss: 0.6407 - val_accuracy: 0.7192 - val_loss: 0.6371\n",
      "Epoch 12/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7458 - loss: 0.6272 - val_accuracy: 0.7458 - val_loss: 0.6238\n",
      "Epoch 13/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7715 - loss: 0.6128 - val_accuracy: 0.7646 - val_loss: 0.6096\n",
      "Epoch 14/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7910 - loss: 0.5972 - val_accuracy: 0.7756 - val_loss: 0.5943\n",
      "Epoch 15/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8038 - loss: 0.5802 - val_accuracy: 0.7886 - val_loss: 0.5777\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - accuracy: 0.7717 - loss: 0.5913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.589163064956665, 0.7726500034332275]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=15)\n",
    "\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwLUAQKkhEPt"
   },
   "source": [
    "**¿Y si el valor a enmascarar no es cero?** Podemos usar...\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "tensorflow.keras.layers.Masking(mask_value=0.0, **kwargs)\n",
    "```\n",
    "\n",
    "mask_value es el valor que queremos \"saltarnos\" o \"enmascarar\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
